---
title: "Explaining Song Popularity"
subtitle: "STAT 420, Summer 2023, UIUC - Final Data Project"
author: "Soumya Nanda, Jonas Jansen, Noam Isachar"
output:
  pdf_document: default
  theme: readable
  toc: yes
  html_document:
urlcolor: cyan
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

------------------------------------------------------------------------

# Introduction

In this analysis project we decided to work on the [Spotify_1Million_Tracks](https://www.kaggle.com/datasets/amitanshjoshi/spotify-1million-tracks) which has popularity and other songs features and measures gathered by Spotify about over 1 Million tracks. Our main goal is to discover what makes a song popular and what impact different characteristics of a songs have on the song's popularity. The model we will build will also be able to predict a song's popularity.

The dataset is published and available on Kaggle and the data was extracted from the Spotify API using the Python library Spotipy. It contains information about over a Million songs from over 60,000 artists and across 82 genres between 2000 and 2023. The dataset also contains some interesting categorical variables which are some facts about each song such as the key (A, C, G#...), the mode (Major or Minor) and the time signature. In addition to those, there are numerical features about each songs which are computed by Spotify algorithms. To name a few, we have the danceability of the song, the energy, the acousticness and the valence, all in the range of 0.0 to 1.0.

Having all this musically detailed and in-depth information about so many songs provides the opportunity to use a linear model for researching what features of a song contribute to it's chance of being popular, how a song should ideally be for it to be popular, and verify the possibility of predicting how popular a song is. Knowing all the above can drive certain decisions and directions when writing a song with the goal of achieving popularity.

# Methods

We start by importing the required libraries.

```{r, warning = FALSE}
library(knitr)
library(ggplot2)
library(GGally)
library(MASS)
library(gridExtra)
library(preprocessCore)
library(lmtest)
library(faraway)
```

Let's have the first glance of the data.

```{r}
data = read.csv("../data/spotify_data.csv")
str(data)
```

## Data Preprocessing

We can get rid of the first column which is the index and also of the "track_id" column which is useless to us. For clarity, let's change the values of "mode" from 0 and 1 to "minor" and "major" and replace the duration from milliseconds to minutes. Finally, we can create factor variables out of some of the columns.

```{r}
data$mode = ifelse(data$mode == 1, "major", "minor")
data$duration_m = data$duration_ms / 1000 / 60

data$year = factor(data$year)
data$genre = factor(data$genre)
data$key = factor(data$key)
data$mode = factor(data$mode)
data$time_signature = factor(data$time_signature)

data = subset(data, select = -c(X, track_id, duration_ms))

str(data)
```

Looks good. The dataset now 18 variables, two of which are the artist name and track name which we won't use in the model but only for exploration and interpretation. That leaves us with the "popularity" variable which we are going to use as the response and 15 other variables that can help us as predictors. 10 of which are numeric and 5 are categorical factor variables.

Now let's check for null values.

```{r}
colSums(is.na(data))
```

There are none.

## Data Exploration and Analysis

### Data Description

We can now look at the summary of the data which shows statistics of the different variables.

```{r}
summary(data)
```

Let's explain the variables we have here. Our goal is to explain and predict the "popularity" variable, which is calculated by an algorithm of Spotify, and according to them it "is based, in the most part, on the total number of plays the track has had and how recent those plays are".

Besides the artist and track name and the year and genre which are obvious, we have some more factual categorical variables:

1.  Key: ranges from 0 to 11 and indicates the key the track is in starting from 0 = C.
2.  Mode: refers to whether the track is in major or minor scale.
3.  Time signature: An estimated time signature which specifies how many beats are in a bar.

The numerical variables mostly contain audio features of the track:

1.  Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.
2.  Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy.
3.  Loudness: The overall loudness of a track in decibels (dB).
4.  Speechiness: Speechiness detects the presence of spoken words in a track.
5.  Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic.
6.  Instrumentalness: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context.
7.  Liveness: Detects the presence of an audience in the recording.
8.  Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
9.  Tempo: The overall estimated tempo of a track in beats per minute (BPM).
10. Duration: The duration of the track in minutes.

More on the variables can be read on the [Spotify API documentation](https://developer.spotify.com/documentation/web-api).

The data looks clean but there is one case of outliers in the duration of tracks. Let's keep track with maximum length of 40 minutes, which is by itself very unusual but we can still think of some songs of that length (The Orb - The Blue Room). In fact, any track longer than 40 minutes has to be considered as an album by the British Phonographic Industry. In addition, it is safe to say that the maximum duration of 100 minutes is not something we want to base our model on as it doesn't realistically reflect real songs.

```{r}
data = data[data$duration_m <= 40, ]
summary(data$duration_m)
```

One other thing we want to verify is the 0 values n tempo.

```{r}
head(data[data$tempo == 0, 1:2], 10)
```

After listening to some of those we can verify that the 0 for tempo are legit values and these songs don't follow a beat.

Another suspicious 0 is for the popularity. Knowing Spotify, there is a lot of junk there that can't really be tagged as songs and it is possible that it ended up in this dataset and it is contaminating the data. Another possibility is that it is a bug in the dataset, since getting zero popularity and not even a fraction above it means that no streams were made. Let's see some of the tracks with zero popularity and manually verify what happened.

```{r}
head(data[data$popularity == 0, 1:2], 10)
```

After carefully checking examples, we noticed some of them have Millions of streams, like Greetings by Joni Haastrup. This makes us think that there is a problem and some songs are added with zero popularity by mistake. Let's see how many songs have zero popularity, and how many have any other number.

```{r}
print(paste("Number of tracks with zero popularity:", nrow(data[data$popularity == 0, ])))
print(paste("Number of tracks with non-zero popularity:", nrow(data[data$popularity > 0, ])))
```

For some reason tracks with zero popularity are over 10% of the dataset, which other than being suspicious is also VERY unbalanced. We decide to remove these observations as there seems to be so many of them, also in cases that it doesn't make sense.

```{r}
data = data[data$popularity > 0, ]
summary(data)
```

### Numerical Variables Analysis

Let's start exploring the numerical variables and the relationships between them.

```{r}
num_vars = sapply(data, is.numeric)
num_data = data[, num_vars]
num_data_sample = num_data[sample(nrow(num_data), 10000), ]

pair_plot_sample = ggpairs(num_data_sample, upper = list(continuous = wrap("cor", size = 6))) +
  theme_bw() + 
  theme(axis.line=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        text = element_text(size = 30))
```

```{r, warning = FALSE, fig.width=25, fig.height=15}
print(pair_plot_sample, progress = F)
```

We sampled 10,000 observations from the dataset for this pair-plot since doing it on the entire dataset would take much more time and also flood the plots with so much dots that they would become all black. A sample of 10,000 is more than enough to show the relationship between variables.

Let's highlight some interesting points:

-   The first thing to observe is how difficult it is to write a popular song. Even after we removed songs with 0 popularity the distribution is still very left-skewed and the mean is only 21.3 out of 100.

-   Looking at the main diagonal where the distribution of each variable is plotted, we can see that many of them including the popularity are skewed which hints that transformations will be needed if we want better prediction results. We will consider log, square-root, inverse and box-cox transformations for this task.

-   In terms of correlation with the response we don't have significant results which makes it more challenging to select predictors.

-   Energy has a strong positive correlation with loudness and a strong negative correlation with acousticness. This suggests that we might want to be careful using all of them in order to avoid collinearity issues. Also danceability and valence have some correlation we should take note of.

-   In general, we might want to avoid using the loudness and tempo variables in our model since some of the algorithm-generated variables are probably based on them in some way.

-   Some other observations jump out which are not related to the model we intend to build. Regarding the instrumentalness, it has some positive correlation with the duration and negative correlation with the loudness and valence, which makes sense. Many of the instrumental songs we know are indeed longer than usual and also have some quiet gloomy parts. The tempo has a positive correlation with the loudness and negative correlation with the acousticness. In reality faster and upbeat songs are usually louder than slower ones, and acoustic songs we know do tend to be slower in BPM. One more relationship that is logical is the positive one between liveness and speechiness. Concerts and live version have some spoken words in them when the artists communicate with the crowd.

After getting the hint that transformations are required, let's run the Box-Cox method on every non-negative numerical variable to see which is the recommended transformation.

```{r}
eps = 1e-8
# Function to calculate the optimal lambda for a variable using Box-Cox
get_optimal_lambda = function(variable) {
  lm_formula = as.formula(paste(variable, "+ eps ~ 1"))
  result = boxcox(lm_formula, data = data, plot=FALSE)
  return(result$x[which.max(result$y)])  # Return the optimal lambda
}

# Create an empty data frame to store the results
table_optimal_lambdas = data.frame(variable = character(), optimal_lambda = numeric(), stringsAsFactors = FALSE)

# Use a loop to get the optimal lambda for each variable
for (var in names(num_vars[num_vars])) {
  if (min(data[[var]]) < 0) {
    optimal_lambda = 1
  }
  else {
    optimal_lambda = get_optimal_lambda(var)
  }
  table_optimal_lambdas = rbind(table_optimal_lambdas, data.frame(variable = var, optimal_lambda = optimal_lambda))
}

# Print the table
print(table_optimal_lambdas)
```

Save the transformed variables with the recommended lambdas into a new dataframe. Then, we plot the pair-plot again and see of the distributions resemble a normal distribution.

```{r}
apply_boxcox_transformation = function(variable, lambda) {
  transformed_variable = if (abs(lambda) < 0.01) {
    log(data[[variable]] + eps)
  } else {
    (data[[variable]]^lambda - 1) / lambda
  }
  return(transformed_variable)
}

categorical_vars = names(data)[sapply(data, function(x) is.factor(x) || is.character(x))]

# Create a new data frame to store the transformed data
boxcox_data = data.frame(
  sapply(names(num_vars[num_vars]), function(var) apply_boxcox_transformation(var, table_optimal_lambdas[table_optimal_lambdas$variable == var, "optimal_lambda"])),
  data[categorical_vars],  # Add the categorical columns from the original data
  stringsAsFactors = FALSE
)
boxcox_data = boxcox_data[names(data)]
```

```{r}
boxcox_num_data = boxcox_data[, num_vars]
boxcox_num_data_sample = boxcox_num_data[sample(nrow(boxcox_num_data), 10000), ]

boxcox_pair_plot_sample = ggpairs(boxcox_num_data_sample, upper = list(continuous = wrap("cor", size = 6))) +
  theme_bw() + 
  theme(axis.line=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        text = element_text(size = 30))
```

```{r, warning = FALSE, fig.width=25, fig.height=15}
print(boxcox_pair_plot_sample, progress = F)
```

The distributions still look off. When Box-Cox transformations are unsatisfactory, quantile normalization emerges as a robust alternative. Unlike Box-Cox, quantile normalization is distribution-free and aligns empirical distributions across variables, ensuring fair comparisons and improved statistical analyses, even with non-normally distributed data. It provides a versatile, data-driven approach to normalize variables, making it an effective choice when dealing with datasets lacking a specific theoretical distribution.

Quantile normalization equalizes the empirical distributions of numerical data across variables by mapping each value to its corresponding quantile in a reference distribution (e.g., uniform or Gaussian). This process ensures that all variables share the same distributional properties, making the data suitable for statistical analyses and comparisons without relying on any specific theoretical distribution.

Let's perform quantile normalization on the numerical variables and the show the pair-plot again.

```{r}
quantile_data = as.data.frame(normalize.quantiles(as.matrix(num_data)))
colnames(quantile_data) = colnames(num_data)
quantile_data = cbind(quantile_data, data[categorical_vars])
quantile_data = quantile_data[names(data)]
```

```{r}
quantile_num_data = quantile_data[, num_vars]
quantile_num_data_sample = quantile_num_data[sample(nrow(quantile_num_data), 10000), ]

quantile_pair_plot_sample = ggpairs(quantile_num_data_sample, upper = list(continuous = wrap("cor", size = 6))) +
  theme_bw() + 
  theme(axis.line=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        text = element_text(size = 30))
```

```{r, warning = FALSE, fig.width=25, fig.height=15}
print(quantile_pair_plot_sample, progress = F)
```

Beautiful. The distributions of the variables are not skewed and the scatterplots resemble what we are used to see when working with normally distributed data. When building the models we will most likely use the quantile normalized data.

One more interesting analysis to try is to bin the popularity into 10 chunks and see the difference in each variable across the different bins of popularity.

```{r, fig.width=20, fig.height=15}
data$popularity_bins = cut(data$popularity, breaks = 10)
plot_list = list()

for (column in names(data)) {
  if (column != "popularity_bins" && column != "popularity" && num_vars[column]) {
    plot = ggplot(data, aes(x = popularity_bins, y = .data[[column]], fill = popularity_bins)) +
      geom_boxplot() +
      labs(x = "Popularity Bins", y = column, title = paste("Distribution of", column, "by Popularity Bins")) +
      theme(axis.text.x = element_blank(), legend.position = "none") +
      facet_grid(~ popularity_bins, scales = "free_x", space = "free_x", switch = "x")
    
    plot_list[[column]] = plot
  }
}

grid.arrange(grobs = plot_list, ncol = 2)
data = subset(data, select = -c(popularity_bins))
```

From the plots above we can see that across all the numerical predictors we have less variance and outliers as the popularity grows. They all stabilize around some range in the higher bins. This is worrying in terms of the constant variance assumption, but we hope the transformations will help mitigate that.

We can see a weak positive trend in the danceability, which resembles the one of the valence. This means that the popular songs tend to be happier and more danceable.

Also the energy and loudness share a similar behavior between them, but the loudness has much less variance. Anyway both of them don't seem to have an observable relationship with the popularity.

Another trend to note is that the instrumentalness and speechiness massively decrease and are almost at 0 when the popularity grows. This reveals another tip for making a popular song: don't make it instrumental are include spoken speech in it.

Maybe the clearest trend above is the negative one between the acousticness and the popularity. There are some popular acoustic songs but it sure doesn't raise your chances.

As for the tempo, its mean is pretty stable across all the bins, but the chance for outliers on both sides are decreasing as the popularity increases. Maybe a polynomial transformation will work here?

Finally, the chance of a long song to be popular is very low as most of the longer songs are in the lower half of the popularity bins.

### Categorical Variables Analysis

Now let's take a look at the impact of the categorical predictors. We start with the genre, and since it has many values and plotting would be difficult, what we can do is check the genres with the best and worst mean popularity.

```{r}
mean_popularity_by_genre = aggregate(popularity ~ genre, data, mean)

best_mean_popularity_by_genre = mean_popularity_by_genre[order(-mean_popularity_by_genre$popularity), ]
worst_mean_popularity_by_genre = mean_popularity_by_genre[order(mean_popularity_by_genre$popularity), ]
```

```{r}
print(head(best_mean_popularity_by_genre, 10))
```

```{r}
print(head(worst_mean_popularity_by_genre, 10))
```

As expected, the genre greatly impacts the popularity. There is a huge difference in the mean popularity across genres.

The only issue with the genre is that there are two many of them, with 82 values. That could be a problem if we decide to interact it with some other variables, which could happen since different genres might have different recipes for success. What we can do is create another variable called "upper_genre" which will be a mapping of the current genres to a higher level genre. We will create the mapping ourselves by carefully listening, researching and examining the current genres and categorizing them into upper genres. We will use the upper genre with caution.

```{r, warning=FALSE}
genre_mapping = list(
  "rock" = c("alt-rock", "garage", "hard-rock", "psych-rock", "rock", "rock-n-roll"),
  "metal" = c("black-metal", "death-metal", "emo", "goth", "grindcore", "hardcore", "metal", "metalcore", "heavy-metal"),
  "electronic" = c("ambient", "breakbeat", "chicago-house", "dance", "dancehall", "deep-house", "detroit-techno",
                   "drum-and-bass", "dub", "dubstep", "edm", "electro", "electronic", "hardstyle", "house",
                   "minimal-techno", "progressive-house", "techno", "trance", "party", "chill", "club", "industrial"),
  "pop" = c("cantopop", "indie-pop", "k-pop", "pop", "power-pop", "pop-film"),
  "country" = "country",
  "folk" = "folk",
  "classical/opera" = c("classical", "opera"),
  "hip-hop" = "hip-hop",
  "blues" = "blues",
  "jazz" = "jazz",
  "soul" = "soul",
  "world" = c("indian", "spanish", "swedish", "tango", "forro", "sertanejo", "salsa", "samba", "french", "german", "afrobeat"),
  "ska" = "ska",
  "punk" = c("punk", "punk-rock"),
  "funk/disco" = c("funk", "disco"),
  "trip-hop" = "trip-hop",
  "acoustic" = c("acoustic", "singer-songwriter", "songwriter", "sad", "guitar", "piano"),
  "new-age" = "new-age",
  "other" = c("comedy", "gospel", "romance", "show-tunes", "sleep", "groove")
)

map_genre = function(genre) {
  for (key in names(genre_mapping)) {
    if (genre %in% unlist(genre_mapping[[key]])) {
      return(key)
    }
  }
}

data$upper_genre = sapply(data$genre, map_genre)
quantile_data$upper_genre = data$upper_genre
```

While handling and categorizing the genres we noticed some inconsistencies with the genre values across different songs. Some completely not related songs were wrongly categorized to the same genre, which makes sense since the genre is an algorithm-based variable. This leads us to think that we want to try a model without considering the genre, not even in the reduced form. Before proceeding let's see the average popularity of the new upper genres.

```{r}
mean_popularity_by_upper_genre = aggregate(popularity ~ upper_genre, data, mean)

best_mean_popularity_by_upper_genre = mean_popularity_by_upper_genre[order(-mean_popularity_by_upper_genre$popularity), ]
worst_mean_popularity_by_upper_genre = mean_popularity_by_upper_genre[order(mean_popularity_by_upper_genre$popularity), ]
```

```{r}
print(head(best_mean_popularity_by_upper_genre, 10))
```

```{r}
print(head(worst_mean_popularity_by_upper_genre, 10))
```

Now we can look at the year variable.

```{r, fig.width=25, fig.height=6}
mean_popularity_by_year = aggregate(data$popularity, by = list(year = data$year), mean)
mean_popularity_by_year = reshape2::melt(mean_popularity_by_year, id.vars = "year")

ggplot(mean_popularity_by_year, aes(x = year, y = value, group = variable, color = variable)) +
  geom_line() +
  geom_point() +
  labs(x = "Year", y = "Mean Popularity", title = "Mean Popularity by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 22)) +
  theme(legend.position = "none")
```

Since the popularity is based also on whether the track was recently played, we see greater popularity to recent songs which makes sense. However, our main goal is to understand what elements of a song have impact on making it popular, and when writing a song it is not possible to change the year it is released in. In other words, the year is not going to help us say what makes a song popular since the popularity is biased towards recent songs and the time is not something an artist controls.

In order to be able to explain the model but also utilize the year in our predictions, we will use the year variable to interact with all the other variables, and then we can select each year and see how the parameters of the other variables change. For example, by interacting the year with the rest we will be able to select the year 2023 and see how a song should be in order to maximize its chances of popularity in 2023.

Despite all that, we are still curious to see the variation in the characteristics of songs across different years, and see what features of songs from different years contribute to its popularity right now. Let's see the mean of each variable as a function of the year, and also a plot of correlation with the popularity by year.

```{r, fig.width=25, fig.height=8}
mean_data = aggregate(data[, num_vars], by = list(year = data$year), mean)
mean_data = subset(mean_data, select = -c(popularity, tempo, duration_m, loudness))
mean_melted = reshape2::melt(mean_data, id.vars = "year")

ggplot(mean_melted, aes(x = year, y = value, group = variable, color = variable)) +
  geom_line() +
  geom_point() +
  labs(x = "Year", y = "Mean Value",
       title = "Mean of [0, 1] Ranged Numerical Variables as a Function of Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 22))
```

```{r, fig.width=25, fig.height=8}
cor_data = data.frame(year = unique(data$year))
cor_data$year = as.numeric(as.character(cor_data$year))

for (var in names(num_vars[num_vars])) {
  if (var != "popularity"){
    cor_values = sapply(unique(data$year), function(y) {
      subset_data = subset(data, as.numeric(as.character(year)) == y)
      cor(subset_data[[var]], subset_data$popularity)
    })
    
    cor_data[var] = cor_values
  }
}

cor_melted = reshape2::melt(cor_data, id.vars = "year")

ggplot(cor_melted, aes(x = year, y = value, group = variable, color = variable)) +
  geom_line() +
  geom_point() +
  labs(x = "Year", y = "Correlation with Popularity",
       title = "Correlation with Popularity Over the Years") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 22))
```

To our surprise, there are no significant trends in the means over the years. The only visible changes are the drop in the valence which generally means that songs became sadder, and the rise and fall of energy that reached a peak around 2010.

In the correlations plot we see a clear rise in the impact danceability and energy have on the popularity, especially after 2020. Groovy energetic songs are becoming more and more likable. The duration went from 0.0 to almost -0.2 at some point, which highlights that popular songs are getting shorter.

Now let's see how the time signature, key and mode affect the popularity.

```{r, fig.height=10}
ts_pop = ggplot(data, aes(x = time_signature, y = popularity, fill=time_signature)) +
  geom_boxplot() +
  theme(legend.position = "none") +
  labs(x = "Time Signature", y = "Popularity", title = "Distribution of Popularity by Time Signature")

key_pop = ggplot(data, aes(x = key, y = popularity, fill=key)) +
  geom_boxplot() +
  theme(legend.position = "none") + 
  labs(x = "Key", y = "Popularity", title = "Distribution of Popularity by Key")

mode_pop = ggplot(data, aes(x = mode, y = popularity, fill=mode)) +
  geom_boxplot() +
  theme(legend.position = "none") + 
  labs(x = "Mode", y = "Popularity", title = "Distribution of Popularity by Mode")

grid.arrange(ts_pop, key_pop, mode_pop, ncol = 1)
```

There are no significant changes in the distribution of the popularity between different values of time signature, key or mode. Maybe we should check the combination of key and mode since they often touch similar characteristics of a song.

```{r, fig.width=16, fig.height=5}
data$key_mode_combination = interaction(data$mode, data$key)

combined_plot = ggplot(data, aes(x = key_mode_combination, y = popularity, fill = mode)) +
  geom_boxplot() +
  theme(legend.position = "none") +
  labs(x = "Key and Mode", y = "Popularity", title = "Distribution of Popularity by Key and Mode") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ key_mode_combination, scales = "free_x", ncol = 24)

print(combined_plot)

data = subset(data, select = -c(key_mode_combination))
```

Still doesn't look significant for the popularity. In terms of the mean and even the outliers the boxplots don't look too different.

There is little chance that we will need the time signature, key or mode in the final model.

### Conclusions

Before building the model, we can lay out a few conclusions from the sections above that will help us in building a better model and more importantly, write a hit song!

-   The distribution of the popularity variable is left-skewed, indicating that most songs have relatively low popularity scores, with only a few reaching higher scores. To tackle that in our model we might want to try transforming the response, together with some other numerical variables that are skewed. After using the Box-Cox method we retrieved the optimal lambdas for each variables and implemented it in a dedicated dataframe. The result was still unsatisfactory so we had to use a more robust way of normalizing data and forcing it into a normal distribution, such as quantile normalization. The normalized data was saved into a dataframe which we are likely to use when fitting models.

-   There is no single variable that is strongly correlated with the popularity. This means that a combination of factors is likely to influence the popularity of a song, and that we will need to try a few interactions.

-   The most popular songs are usually short, upbeat, energetic, not instrumental or acoustic and do not include spoken words.

-   There are some potential collinearity issues we should avoid. Energy, loudness and acousticness have some relationship between them (acousticness seems to impact the popularity the most out of them), and also danceability and valence correlate. We should be careful using loudness and tempo because they are most probably used to calculate other variables.

-   The genre is a problematic variable. The average popularity across different genres is variant, so it has some importance. Together with that, there a many different genres which could make the models complex. That's why we introduced the reduced upper genre. Also there are many errors with the genre since it was created by a Spotify algorithm, possibly using the other variables we have in the dataset.

-   Because of the way the popularity is computed, the year is very important as well. However, we want to be able to say how to write a song for it to be popular, and since the year of release is not something you control we would prefer to interact that variable with the numerical variables we choose to include, which will allow us to specify the recipe for popularity on each year.

-   The time signature, key and mode don't seem to affect the popularity so much.

Now we can proceed and build some models.

## Model Building

### Define Test Criteria

Before we start fitting some models, we split the data into test and train data. This helps us evaluating the models, also towards overfitting. The chosen ratio is 70% train data and 30% test data. We do this for both the quantile transformed data and the original data set.

```{r}
set.seed(42)
ratio = 0.7

# normal data set
data_fit = subset(data, select = -c(artist_name, track_name))

sample_size = floor(ratio * nrow(data_fit))
data_idx = sample(nrow(data_fit), sample_size)
data_trn = data_fit[data_idx, ]
data_tst = data_fit[-data_idx, ]

# quantile normalized data set
quantile_data_fit = subset(quantile_data, select = -c(artist_name, track_name))

sample_size = floor(ratio * nrow(quantile_data_fit))
quantile_data_idx = sample(nrow(quantile_data_fit), sample_size)
quantile_data_trn = quantile_data_fit[quantile_data_idx,]
quantile_data_tst = quantile_data_fit[-quantile_data_idx, ]
```

To make the evaluation of the fitted models easier, we define a function which is calculating the most important tests for us. The function receives the model and the test dataset as parameters. The first test is the Shapiro-Wilk test of normality. Note that the shaprio.test function in R is limited to accept max. 5000 residuals. Since the original dataset contains 1 million tracks, these are too many residuals even after splitting the dataset into test and train. As a workaround, a random sample size of 5000 residuals is picked. This makes the result of that test somewhat unreliable, since it can get a biased sample. But the value should still give us an idea how good or bad the model might be.

Further test criteria are the Breusch-Pagan test for checking the constant variance assumption, loocv_rmse for cross validation the model and also RMSE and MAE calculated from the given test data. The models Adjusted R-Squared values are returned too.

```{r}
check_model = function(model, test_data){
  model_info = summary(model)
  set.seed(42)
  
  # sample for shapiro
  sample_idx = sample(length(resid(model)), 5000)
  residuals_sample = resid(model)[sample_idx]
  
  # shapiro
  shapiro_p_value =  shapiro.test(residuals_sample)$p.value
  
  # bp test
  bptest_p_value = bptest(model)$p.value
  
  # loocv_rmse
  loocv_rmse = sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))

  # rsme and mae using the test data
  predictions = predict(model, newdata = test_data)
  rsme_test = sqrt(mean((test_data$popularity - predictions)^2))
  mae_test = mean(abs(test_data$popularity - predictions))

  # Creating a data frame to store the results
  results = data.frame(
    adjusted_r_squared = model_info$adj.r.squared,
    rsme_test = rsme_test,
    mae_test = mae_test,
    loocv_rmse = loocv_rmse,
    shapiro = shapiro_p_value,
    bptest = bptest_p_value
  )

  results
}
```

Further a function for plotting the Fitted vs Residuals Plot and the Q-Q Plot is defined.

```{r}
plot_model = function(model) {
  # Fitted vs Residuals Plot
  plot(fitted(model), resid(model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals Plot")
  abline(h = 0, col = "darkorange", lwd = 2)
  
  # Q-Q Plot
  qqnorm(resid(model), main = "Q-Q Plot", col = "darkgrey")
  qqline(resid(model), col = "dodgerblue", lwd = 2)
}
```

### Full Additive Model

Now the data is prepared and the tests are defined. We can start fitting. The first model we build is a full additive one including all predictors. One exception is on the upper genre, since this should be a replacement for genre in a better explainable and useful model. And we choose the non transformed data set. This should give us a base to how much we need to do improving the model

```{r}
model_add_full = lm(popularity ~ . -upper_genre, data = data_trn)
```

Let's run the tests and see how well the model performs.

```{r}
model_add_full_check = check_model(model_add_full, data_tst)
rownames(model_add_full_check) = c("Fully Additive No Transformations")
model_add_full_check
```

```{r}
plot_model(model_add_full)
```

On the first look, the model itself looks significant. Nearly all variables are significant, only a few categorical predictors like genredisco or key9 are not. However, both the constant variance assumption and the normal distribution assumptions are highly suspicious.

We need to adjust our model further. As we suspected before, using transformations and avoiding collinear predictors is necessary.

### Data Transformation

The next step is to focus on the very suspect normality and constant variance assumptions of the full additive model. The first thing changed was using transformed data. Box-Cox transformation led to worse results than using quantiles. Hence we did not investigate any further in that direction. Additional transformations were applied on some predictors. Options were square root, polynomials, log, exp and inverse. Valence and Loudness were improved using exp, but using both made the model worse so we chose loudness here. We found transforming duration_m with exp helpful too.

```{r}
formula = "popularity~danceability+energy+exp(loudness)+speechiness+acousticness+instrumentalness+liveness+valence+tempo+exp(duration_m)+key+mode+time_signature+genre"
model_add_quant_trans = lm(formula, data = quantile_data_trn)
```

```{r}
model_add_quant_trans_check = check_model(model_add_quant_trans, quantile_data_tst)
rownames(model_add_quant_trans_check) = c("Additive Qunatile Normalized")
model_add_quant_trans_check
```

```{r}
plot_model(model_add_quant_trans)
```

This model looks better regarding normal the normal assumption. The Q-Q plot is not perfect but way better. The Shapiro-Wilk test failed to reject for sigma 0.05. But as said this might be biased due to the sample size. No visible improvements could be achieved towards the constant variance assumption. It is still bad. The RMSE stayed the same, but LOOCV RMSE looks way better. On contrary, the Adjusted R Squared got worse. Trying to reduce the predictors with the help of AIC or BIC in a backward search did not work out. The predictor number stayed the same.

### Collinearity

As a next step, we check for collinearity issues. We will use the transformed model and calculate the vif. We define vif values higher than 5 as problematic.

```{r}
formula = "popularity~danceability+energy+exp(loudness)+speechiness+acousticness+instrumentalness+liveness+valence+tempo+exp(duration_m)+key+mode+time_signature+genre"
model_add_quant_vif = lm(formula, data = quantile_data_trn)
```

```{r}
vif_values = faraway::vif(model_add_quant_vif)
(problematic_variables = names(vif_values[vif_values > 5]))
```

We see that the factor variable time_signature has vif values above the threshold of 5. This is problematic, it indicates that those variables are contributing to multicollinearity in our regression model. Therefore we remove them and do the test again.

```{r}
formula = "popularity~danceability+energy+exp(loudness)+speechiness+acousticness+instrumentalness+liveness+valence+tempo+exp(duration_m)+key+mode+genre"
model_add_quant_no_vif = lm(formula, data = quantile_data_trn)
```

```{r}
vif_values = faraway::vif(model_add_quant_no_vif)
(problematic_variables = names(vif_values[vif_values > 5]))
```

```{r}
model_add_quant_no_vif_check = check_model(model_add_quant_no_vif, quantile_data_tst)
rownames(model_add_quant_no_vif_check) = c("Additive Qunatile Normalized No VIF")
model_add_quant_no_vif_check
```

```{r}
plot_model(model_add_quant_no_vif)
```

Now all predictors are below the threshold. No further changes are required towards the constant variance and normality assumptions. The means we got a simpler and better explainable model with likely the same quality in terms of prediction.

### Interaction

In the data analysis we stated that interactions might be helpful to better explain the popularity. In this step we will evaluate that. Some interactions were added to the previous model, to see if we can get a better model.

```{r}
formula = "popularity~danceability+energy+exp(loudness)+speechiness+acousticness+instrumentalness+liveness+valence+tempo+exp(duration_m)+key+mode+time_signature+genre+danceability:genre+speechiness:liveness+valence:tempo+energy:tempo+acousticness:instrumentalness+danceability:liveness"
model_int_quant_trans = lm(formula, data = quantile_data_trn)
```

```{r}
model_int_quant_trans_check = check_model(model_int_quant_trans, quantile_data_tst)
rownames(model_int_quant_trans_check) = c("Interaction Qunatile Normalized")
model_int_quant_trans_check
```

```{r}
plot_model(model_int_quant_trans)
```

The model performs slightly better in terms of meeting the normal assumption. The other test values do not vary. Therefore we get a slightly better model for the cost of explainability.

### Explainability

Our goal is to achieve a simpler healthy model that will help us in explaining the popularity better while getting decent RMSE and MAE on the test set. Previously we suspected that the genre is produced by an algorithm that is assessing a song using some features we see here, so we will not use any genre predictor as it might be confusing. Also, we saw that the key, mode and time signature are not changing the average popularity so much so we will try to drop them as well. Finally, we will handpick a few predictors that were the most promising and not correlated between them, and interact them with the year to create coefficients for each year.

```{r}
model_explain = lm(popularity ~ year * (danceability + acousticness + instrumentalness + speechiness + liveness + duration_m), 
                   data = quantile_data_trn)
```

```{r}
model_explain_check = check_model(model_explain, quantile_data_tst)
rownames(model_explain_check) = c("Explainable By Year")
model_explain_check
```

```{r}
plot_model(model_explain)
```

# Results

In order to compare the different models diagnostics and performance, let's view them in a table.

```{r}
check_results = rbind(model_add_full_check, model_add_quant_trans_check, model_add_quant_no_vif_check, model_int_quant_trans_check, model_explain_check)
kable_table = kable(check_results, format = "html", caption = "Model Test Results")
kable_table
```

One observation to make is that all the models failed the Shapiro-Wilk and Breusch-Pagan tests. Despite our efforts and experiments, the data is very challenging. However, we can see in the Q-Q and fitted vs. residuals plots that the different models do differ in their violations of the assumptions, which we will take into account.

All the models have their advantages and disadvantages, and we need to keep our goal in mind. In the first additive model we see good predictive results but the diagnostic plots are very bad. The model uses variables with collinearity and no transformations. The next two models tackle these issues one by one but as in the case of the additive model, they are probably overfitting based on the genre. We can see that the coefficients of the genres are higher than all, and they don't help us in the quest of explainability. Writing a pop song as a conclusion doesn't help much. We also considered interactions between some numerical variables but we can see that they don't contribute much to the model's predictive abilities while significantly making it non-interpretable. In the interaction model we tried using the upper genre that we introduced earlier but it raised the same issues of overfitting and no explainability that the genre did.

This leaves us with the final model, the one that interacts the year with some numerical variables. We picked the numerical variables based on the data analysis we did earlier, while focusing on the algorithm-generated ones to avoid collinearity issues by using loudness and tempo. Using this model let's us see how different predictors affect the popularity and also check this for every year to see the trends in the impact of each predictor. Excluding any genre variable removes the bias the model could have towards specific genres. In addition, we can see that the diagnostic plots of this model look better than the ones of the other models with the residuals centered around 0 especially for higher popularity. This makes sense due to the large amount of songs having low popularity for different reasons.

We can visualize the predictions against the actual transformed values.

```{r}
# Set the seed for reproducibility
set.seed(15)

# Sample 500 rows from quantile_data_tst
sample <- quantile_data_tst[sample(nrow(quantile_data_tst), 200), ]

# Make predictions using the model
predictions <- predict(model_explain, newdata = sample)

# Create a scatter plot
plot(sample$popularity, predictions, xlab = "Actual Values", ylab = "Predictions",
     main = "Test Sample Predictions vs. Actual Values", col = "black")

# Add the linear line for the ideal fit (orange line)
abline(a = 0, b = 1, col = "orange", lwd = 3)

# Fit and add the linear line for the actual points (purple line)
lm_fit <- lm(predictions ~ sample$popularity)
abline(lm_fit, col = "purple", lwd = 3)

# Add a legend
legend("bottomright", legend = c("Ideal Linear Fit", "Actual Model Fit"),
       col = c("orange", "purple"), lty = c("solid", "solid"), lwd = 2, cex = 0.8)

```

We don't have a perfect fit but according to us, considering the challenging and unmeasurable subject of prediction, these are sufficiently good predictions and they justify the explanations and conclusions we will make from this model.

Let's use this model and look at the results and more specifically, the coefficients across the years.

```{r}
coefficients = model_explain$coefficients

num_preds = c("danceability", "acousticness", "instrumentalness", "speechiness", "liveness", "duration_m")
coefs_by_year = data.frame(matrix(nrow = length(num_preds), ncol = length(unique(data$year))))
rownames(coefs_by_year) = num_preds
colnames(coefs_by_year) = sort(unique(data$year))

for (pred in num_preds) {
  for (year in unique(data$year)) {
    base_coef = coefficients[pred]
    if (year == 2000) {
      coefs_by_year[pred, year] = base_coef
    }
    else {
      coefs_by_year[pred, year] = base_coef + coefficients[paste0("year", year, ":", pred)]
    }
  }
}

coefs_by_year
```

```{r, fig.width=25, fig.height=6}
melted_coefs = reshape2::melt(t(coefs_by_year), variable.name = "year", value.name = "coef")
ggplot(melted_coefs, aes(Var1, coef, group=factor(Var2), color = Var2)) +
  geom_line() +
  geom_point() +
  labs(x = "Year", y = "Coefficient",
       title = "Model Coefficients by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 22))
```

# Discussion

The final model is very useful to us, because it allows as to interpret the effect of each selected variable across the different years in the dataset. In addition to explaining a few coefficients and how they impact the popularity, we can look at each variable's changing impact across the past 24 years. This is the most explainability we could hope for when in the beginning of this report.

Let's look at the plot above and draw some conclusions:

-   Across the chart, the most significant variable is the danceability. If you want a song to really succeed this is the way to go. In addition to that, we even see an increase in the significance starting from 2012. Maybe this is because danceable songs became easier to make when home studios and equipment became better and more affordable, so they also had to bring something more to the table and improve the standard of danceable songs?

-   Similarly to the danceability, acousticness positively affected the popularity with a trend starting around 2012, with the difference that the trend completely changed 2022. After 10 years of general increase in the positive impact on the popularity, acousticness now actually negatively impacts the popularity. Are we at the start of 10 bad years for acoustic songs?

-   Liveness and speechiness are pretty stable around zero. That means that even though we thought before that a live song with speech in it will reduce the chance of the song becoming popular, we now see it won't. Maybe the previous insight was because of the small amount of songs with high values in these two variables, which made the average value of them low across popular songs. The truth is that the average liveness and speechiness of songs is low across all the levels of popularity, and now we certify that with the coefficients.

-   Another variable which is pretty stable is the instrumentalness. It has a small but still negative coefficient across all the years. People like words to sing and remember from songs.

-   The duration gained a negative momentum in 2015. Until then it was neutral, but since then shorter songs have a higher chance to become popular. This corresponds with a similar trends in other mediums such as videos, where short videos gained popularity in platforms like Vine and later on TikTok.

If only we had two more variables, talent and luck, and we could explain 100% of the popularity...

# Appendix

The report was composed and written by Soumya Nanda, Jonas Jansen and Noam Isachar.
